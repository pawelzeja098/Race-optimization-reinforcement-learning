{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e63381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from envs.filtr_json_from_race import load_from_db\n",
    "import sqlite3\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988ee96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStatePredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,n_steps_ahead, num_layers=1):\n",
    "        super(LSTMStatePredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size * n_steps_ahead)\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "        nn.Linear(hidden_size, 2),  # progress\n",
    "        nn.Linear(hidden_size, 1),  # fuel\n",
    "        nn.Linear(hidden_size, 4),  # wear\n",
    "        nn.Linear(hidden_size, 4),  # temp\n",
    "        nn.Linear(hidden_size, 1)   # track wetness\n",
    "        ])\n",
    "\n",
    "        self.scaler_X = None\n",
    "        self.scaler_Y = None\n",
    "        self.n_steps_ahead = n_steps_ahead\n",
    "        self.output_size = output_size\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     h_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "    #     c_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "    #     out, _ = self.lstm(x, (h_0, c_0))\n",
    "\n",
    "    #     # Bierzemy ukryty stan z ostatniego kroku czasowego\n",
    "    #     last_hidden = out[:, -1, :]  # shape [batch, hidden_size]\n",
    "\n",
    "    #     # Każdy head przewiduje własną grupę cech\n",
    "    #     outputs = [head(last_hidden) for head in self.heads]  \n",
    "\n",
    "    #     # Łączymy wszystko w jeden wektor\n",
    "    #     combined = torch.cat(outputs, dim=1)  # [B, 12]\n",
    "\n",
    "      \n",
    "    #     combined = combined.unsqueeze(1).repeat(1, self.n_steps_ahead, 1)\n",
    "        \n",
    "\n",
    "    #     return combined\n",
    "    # def forward(self, x, h_c=None):\n",
    "    #     # h_c - opcjonalny stan ukryty (batch_first=True)\n",
    "    #     out, h_c = self.lstm(x, h_c)\n",
    "    #     last_hidden = out[:, -1, :]\n",
    "    #     outputs = [head(last_hidden) for head in self.heads]\n",
    "    #     combined = torch.cat(outputs, dim=1)\n",
    "    #     return combined, h_c\n",
    "    \n",
    "    def forward(self, x, h_c=None):\n",
    "        if h_c is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            h_c = (h_0, c_0)\n",
    "        \n",
    "        out, h_c = self.lstm(x, h_c)  # h_c = (h_n, c_n)\n",
    "        last_hidden = out[:, -1, :]\n",
    "        outputs = [head(last_hidden) for head in self.heads]\n",
    "        combined = torch.cat(outputs, dim=1).unsqueeze(1)  # shape [B,1,12]\n",
    "        return combined, h_c\n",
    "\n",
    "def create_scalers(X,Y):\n",
    "\n",
    "    cont_indices_x = slice(0, 19)   # continuous columns for X (0–18)\n",
    "    cont_indices_y = slice(0, 12)   # continuous columns for Y (0–11)\n",
    "\n",
    "    # Scale continuous features\n",
    "    flat_x = np.vstack([x[:, cont_indices_x] for x in X])\n",
    "    flat_y = np.vstack([y[:, cont_indices_y] for y in Y])\n",
    "\n",
    "    scaler_X = MinMaxScaler().fit(flat_x)\n",
    "    # scaler_Y = MinMaxScaler().fit(flat_y)\n",
    "    scaler_Y = StandardScaler().fit(flat_y)\n",
    "    return scaler_X, scaler_Y\n",
    "\n",
    "\n",
    "def scale_input(X, Y, scaler_X, scaler_Y):\n",
    "    cont_indices_x = slice(0, 19)   # continuous columns for X\n",
    "    cont_indices_y = slice(0, 12)   # continuous columns for Y\n",
    "\n",
    "    X_scaled_grouped = []\n",
    "    Y_scaled_grouped = []\n",
    "\n",
    "    for x_seq, y_seq in zip(X, Y):\n",
    "        x_scaled = np.array(x_seq, dtype=float)\n",
    "        x_scaled[:, cont_indices_x] = scaler_X.transform(x_seq[:, cont_indices_x])\n",
    "        X_scaled_grouped.append(x_scaled)\n",
    "\n",
    "        y_scaled = np.array(y_seq, dtype=float)\n",
    "        y_scaled[:, cont_indices_y] = scaler_Y.transform(y_seq[:, cont_indices_y])\n",
    "        Y_scaled_grouped.append(y_scaled)\n",
    "\n",
    "    # Conversion to torch tensors\n",
    "    # X_t = [torch.tensor(x, dtype=torch.float32) for x in X_scaled_grouped]\n",
    "    # Y_cont_t = [torch.tensor(y[:, cont_indices_y], dtype=torch.float32) for y in Y_scaled_grouped]\n",
    "\n",
    "    return X_scaled_grouped, Y_scaled_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5230ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_db():\n",
    "    \n",
    "    \"\"\"\n",
    "    Load data so that each race is a separate sequence:\n",
    "    X = [ [state1_race1, state2_race1, ...], [state1_race2, ...] ]\n",
    "    Y = [ [next1_race1, next2_race1, ...], ... ]\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(\n",
    "        \"E:/pracadyp/Race-optimization-reinforcement-learning/data/db_states_for_regress/race_data_states.db\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT race_id, states_json FROM races ORDER BY race_id\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for race_id, states_json in rows:\n",
    "        states = json.loads(states_json)\n",
    "        data.append(states)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_windows(sequence_x, sequence_y, window_size, n_steps_ahead=5):\n",
    "    X, Y = [], []\n",
    "    for t in range(1, len(sequence_x)):\n",
    "        start = max(0, t - window_size)\n",
    "        window = sequence_x[start:t]\n",
    "\n",
    "        # padding na początku, jeśli okno krótsze niż window_size\n",
    "        pad_len = window_size - len(window)\n",
    "        if pad_len > 0:\n",
    "            window = np.vstack([np.zeros((pad_len, sequence_x.shape[1])), window])\n",
    "        X.append(window)\n",
    "\n",
    "        # Y: wypełniamy zerami, jeśli końcówka wyścigu ma mniej niż n_steps_ahead\n",
    "        y_window = sequence_y[t:t+n_steps_ahead]\n",
    "        if y_window.shape[0] < n_steps_ahead:\n",
    "            pad = np.zeros((n_steps_ahead - y_window.shape[0], sequence_y.shape[1]))\n",
    "            y_window = np.vstack([y_window, pad])\n",
    "        Y.append(y_window)\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def create_x_y(data):\n",
    "    X_grouped, Y_grouped = [], []\n",
    "\n",
    "    for race in data:\n",
    "        X_seq, Y_seq = [], []\n",
    "        for i in range(len(race) - 1):\n",
    "            X_seq.append(race[i][:-2])\n",
    "            Y_seq.append(race[i + 1][:-27]) \n",
    "        \n",
    "        # dodajemy każdy wyścig osobno\n",
    "        X_grouped.append(np.array(X_seq, dtype=float))\n",
    "        Y_grouped.append(np.array(Y_seq, dtype=float))\n",
    "\n",
    "    return X_grouped, Y_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28537823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 12\n",
      "Testing weight: [0.5, 1.8, 3.0, 0.1, 1.5]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     66\u001b[39m Y_train_raw = [np.array(y, copy=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_train]  \u001b[38;5;66;03m# zapisz oryginalne (nieprzeskalowane)\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Y_train_raw = Y_train.copy()  # zachowaj surowe Y_train przed skalowaniem\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m scaler_X, scaler_Y = \u001b[43mcreate_scalers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m X_train, Y_train = scale_input(X_train,Y_train,scaler_X,scaler_Y)\n\u001b[32m     74\u001b[39m X_test, Y_test = scale_input(X_test,Y_test,scaler_X,scaler_Y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mcreate_scalers\u001b[39m\u001b[34m(X, Y)\u001b[39m\n\u001b[32m     63\u001b[39m cont_indices_y = \u001b[38;5;28mslice\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m12\u001b[39m)   \u001b[38;5;66;03m# continuous columns for Y (0–11)\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Scale continuous features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m flat_x = np.vstack([\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcont_indices_x\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n\u001b[32m     67\u001b[39m flat_y = np.vstack([y[:, cont_indices_y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y])\n\u001b[32m     69\u001b[39m scaler_X = MinMaxScaler().fit(flat_x)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "data = load_data_from_db()\n",
    "# input_size = X[0].shape[1]\n",
    "# output_size = Y[0].shape[1]\n",
    "# random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "X, Y = create_x_y(data)\n",
    "input_size = X[0].shape[1]\n",
    "output_size = Y[0].shape[1]\n",
    "print(input_size, output_size)\n",
    "\n",
    "# scaler_X, scaler_Y = create_scalers(X,Y)\n",
    "\n",
    "# X_scaled, Y_scaled = scale_input(X,Y,scaler_X,scaler_Y)\n",
    "\n",
    "# X_scaled = [scaler_X.transform(np.array(race)) for race in X]\n",
    "# Y_scaled = [scaler_Y.transform(np.array(race)) for race in Y]\n",
    "\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "all_fold_train = []\n",
    "all_fold_test = []\n",
    "all_fold_r2_test = []\n",
    "i = 0\n",
    "\n",
    "# lrs = [1e-3,1e-3,1e-3, 7e-4, 5e-4, 3e-4, 1e-4, 7e-5, 5e-5, 3e-5, 1e-5]\n",
    "lr = 1e-3\n",
    "# lr = 7e-5\n",
    "# lr = 5e-4\n",
    "# batch_sizes = [32, 64, 128, 256, 512]\n",
    "batch_size = 128\n",
    "# num_epochs = [50, 100, 150, 200, 300, 400]\n",
    "num_epochs = 75\n",
    "weights = [\n",
    "    # [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    # [0.8, 1.2, 1.5, 0.5, 1.0],\n",
    "    [0.7, 1.4, 2.0, 0.3, 1.2],\n",
    "    [0.6, 1.6, 2.5, 0.2, 1.3],\n",
    "    [0.5, 1.8, 3.0, 0.1, 1.5],\n",
    "    [1.0,1.0,2.5,0.1,0.4]\n",
    "]\n",
    "weight = [0.5, 1.8, 3.0, 0.1, 1.5]\n",
    "# weight = weights[4]  # wybierz zestaw wag do testowania\n",
    "# for weight in weights:\n",
    "\n",
    "print(\"Testing weight:\", weight)\n",
    "for fold, (train_idx, test_idx) in enumerate(loo.split(X)):\n",
    "    \n",
    "    if fold > 0:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    X_test  = [X[i] for i in test_idx]\n",
    "    Y_train = [Y[i] for i in train_idx]\n",
    "    Y_test  = [Y[i] for i in test_idx]\n",
    "\n",
    "    Y_train_raw = [np.array(y, copy=True) for y in Y_train]  # zapisz oryginalne (nieprzeskalowane)\n",
    "\n",
    "\n",
    "    # Y_train_raw = Y_train.copy()  # zachowaj surowe Y_train przed skalowaniem\n",
    "\n",
    "    scaler_X, scaler_Y = create_scalers(X_train,Y_train)\n",
    "\n",
    "    X_train, Y_train = scale_input(X_train,Y_train,scaler_X,scaler_Y)\n",
    "    X_test, Y_test = scale_input(X_test,Y_test,scaler_X,scaler_Y)\n",
    "    n_steps_ahead = 5  # number of future steps to predict\n",
    "\n",
    "\n",
    "    all_X, all_Y = [], []\n",
    "    for race_x, race_y in zip(X_train, Y_train):  \n",
    "        X_r, Y_r = create_windows(race_x, race_y, window_size=30, n_steps_ahead=n_steps_ahead)\n",
    "        all_X.append(X_r)\n",
    "        all_Y.append(Y_r)\n",
    "\n",
    "    X_train = np.vstack(all_X)  # shape: [N_samples, window_size, n_features]\n",
    "    Y_train = np.vstack(all_Y) \n",
    "    all_X, all_Y = [], []\n",
    "    for race_x, race_y in zip(X_test, Y_test):  \n",
    "        X_r, Y_r = create_windows(race_x, race_y, window_size=30)\n",
    "        all_X.append(X_r)\n",
    "        all_Y.append(Y_r)\n",
    "    X_test = np.vstack(all_X)  # shape: [N_samples, window_size, n_features]\n",
    "    Y_test = np.vstack(all_Y)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model = LSTMStatePredictor(input_size=input_size, hidden_size=128, output_size=output_size,n_steps_ahead=n_steps_ahead, num_layers=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "    loss_cont = nn.MSELoss()\n",
    "    loss_cat  = nn.CrossEntropyLoss()\n",
    "\n",
    "    fold_train_losses = []\n",
    "    fold_test_losses = []\n",
    "    # num_epochs = 100\n",
    "    all_r2_per_output = []\n",
    "    fold_r2_scores = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # weights = torch.tensor(\n",
    "        #     [0.8, 0.8, 2.0, 1.5, 1.5, 1.5, 1.5, 0.1, 0.1, 0.1, 0.1, 3.0],\n",
    "        #     dtype=torch.float32,\n",
    "        #     device=device\n",
    "        # )\n",
    "\n",
    "        # for x_batch, y_batch in train_loader:\n",
    "        #     optimizer.zero_grad()\n",
    "        #     pred = model(x_batch)\n",
    "            \n",
    "        #     # bierzemy ostatni krok z predykcji (lub całość, jeśli tak trenujesz)\n",
    "        #     pred_flat = pred[:, -1, :]\n",
    "        #     y_flat = y_batch[:, -1, :]\n",
    "        #     # rozbijanie po grupach\n",
    "        #     loss_progress = loss_cont(pred_flat[:, 0:2], y_flat[:, 0:2])\n",
    "        #     loss_fuel     = loss_cont(pred_flat[:, 2:3], y_flat[:, 2:3])\n",
    "        #     loss_wear     = loss_cont(pred_flat[:, 3:7], y_flat[:, 3:7])\n",
    "        #     loss_temp     = loss_cont(pred_flat[:, 7:11], y_flat[:, 7:11])\n",
    "        #     loss_wet      = loss_cont(pred_flat[:, 11:], y_flat[:, 11:])\n",
    "        #     # łączymy straty z różnych grup z różnymi wagami\n",
    "        #     loss =  weight[0] * loss_progress + \\\n",
    "        #             weight[1] * loss_fuel + \\\n",
    "        #             weight[2] * loss_wear + \\\n",
    "        #             weight[3] * loss_temp + \\\n",
    "        #             weight[4] * loss_wet\n",
    "        #     # łączenie z wagami\n",
    "        #     # loss = (0.8 * loss_progress +\n",
    "        #     #         1.2 * loss_fuel +\n",
    "        #     #         1.5 * loss_wear +\n",
    "        #     #         0.5 * loss_temp +\n",
    "        #     #         1.0 * loss_wet)\n",
    "\n",
    "        #     # standardowy update\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     total_loss += loss.item()\n",
    "        tf_prob = 0.7  # prawdopodobieństwo użycia prawdziwej wartości\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size, seq_len, _ = x_batch.shape\n",
    "            h_c = None\n",
    "            x_input = x_batch.clone()  # startowe okno 30 kroków\n",
    "            loss = 0.0\n",
    "\n",
    "            for step in range(n_steps_ahead):  # generujemy 5 kroków\n",
    "                y_pred, h_c = model(x_input, h_c)  # predykcja 1 kroku\n",
    "                y_true_step = y_batch[:, step, :]\n",
    "\n",
    "                # rozbijamy na grupy i liczymy stratę\n",
    "                loss_progress = loss_cont(y_pred[:, 0:2], y_true_step[:, 0:2])\n",
    "                loss_fuel     = loss_cont(y_pred[:, 2:3], y_true_step[:, 2:3])\n",
    "                loss_wear     = loss_cont(y_pred[:, 3:7], y_true_step[:, 3:7])\n",
    "                loss_temp     = loss_cont(y_pred[:, 7:11], y_true_step[:, 7:11])\n",
    "                loss_wet      = loss_cont(y_pred[:, 11:], y_true_step[:, 11:])\n",
    "                step_loss = weight[0]*loss_progress + weight[1]*loss_fuel + weight[2]*loss_wear + weight[3]*loss_temp + weight[4]*loss_wet\n",
    "                loss += step_loss\n",
    "\n",
    "                # Teacher forcing: decydujemy co wstawimy do okna na kolejny krok\n",
    "                use_teacher = torch.rand(batch_size, device=x_batch.device) < tf_prob\n",
    "                use_teacher = use_teacher.unsqueeze(1).float()  # shape [B,1]\n",
    "                y_next_input = use_teacher * y_true_step + (1 - use_teacher) * y_pred\n",
    "\n",
    "                # przesuwamy okno o 1 krok (autoreg)\n",
    "                x_input = torch.cat([x_input[:, 1:, :], y_next_input.unsqueeze(1)], dim=1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        fold_train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_test = model(X_test_tensor)\n",
    "            test_loss = loss_cont(pred_test, Y_test_tensor).item()\n",
    "            fold_test_losses.append(test_loss)\n",
    "        \n",
    "            scheduler.step(test_loss)\n",
    "            print(f\"Epoch {epoch+1}, current lr: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "            \n",
    "            if epoch == num_epochs - 1:\n",
    "                b, t, f = pred_test.shape\n",
    "                pred_test_flat = pred_test.cpu().numpy().reshape(b*t, f)\n",
    "                Y_test_flat = Y_test_tensor.cpu().numpy().reshape(b*t, f)\n",
    "\n",
    "                # inverse transform\n",
    "                pred_test_inv = scaler_Y.inverse_transform(pred_test_flat)\n",
    "                Y_test_inv = scaler_Y.inverse_transform(Y_test_flat)\n",
    "\n",
    "                # R² po cechach\n",
    "                r2s = [r2_score(Y_test_inv[:, i], pred_test_inv[:, i]) for i in range(f)]\n",
    "                avg_r2_per_output = np.mean(r2s)\n",
    "                print(\"Avg R2 per output:\", r2s)\n",
    "                print(\"Mean R2:\", avg_r2_per_output)\n",
    "\n",
    "                # === DIAGNOSTYKA PER-FEATURE ===\n",
    "                from sklearn.metrics import mean_squared_error\n",
    "                print(\"\\n--- DIAGNOSTYKA ---\")\n",
    "\n",
    "                # przygotuj dane surowe z treningu (przed skalowaniem!)\n",
    "                # jeśli chcesz pełną dokładność, przechowuj Y_train_raw = Y_train (niezeskalowane) tuż po wczytaniu\n",
    "                # tu tymczasowo przyjmujemy, że masz Y_train nieprzeskalowane dostępne jako oryginalne dane:\n",
    "                flat_y_train = np.vstack([y for y in Y_train_raw])  # jeśli Y_train jest już scaled, zmień to na surowe Y_train_raw\n",
    "\n",
    "                vars_train = flat_y_train.var(axis=0)\n",
    "                means_train = flat_y_train.mean(axis=0)\n",
    "                mins = flat_y_train.min(axis=0)\n",
    "                maxs = flat_y_train.max(axis=0)\n",
    "\n",
    "                print(\"Feature | var | mean | min | max\")\n",
    "                for i in range(f):\n",
    "                    print(i, round(vars_train[i],6), round(means_train[i],6), round(mins[i],6), round(maxs[i],6))\n",
    "\n",
    "                # baseline R² (predykcja średnią z train)\n",
    "                mean_pred = np.tile(means_train.reshape(1,-1), (Y_test_inv.shape[0],1))\n",
    "                baseline_r2 = [r2_score(Y_test_inv[:,i], mean_pred[:,i]) for i in range(f)]\n",
    "                print(\"\\nBaseline R2 per feature:\", baseline_r2)\n",
    "\n",
    "                # model R² + MSE + min/max pred\n",
    "                for i in range(f):\n",
    "                    r2 = r2_score(Y_test_inv[:,i], pred_test_inv[:,i])\n",
    "                    mse = mean_squared_error(Y_test_inv[:,i], pred_test_inv[:,i])\n",
    "                    print(f\"feat {i}: R2={r2:.4f}, MSE={mse:.6f}, pred_min={pred_test_inv[:,i].min():.4f}, pred_max={pred_test_inv[:,i].max():.4f}\")\n",
    "                print(\"--- KONIEC DIAGNOSTYKI ---\\n\")\n",
    "\n",
    "                num_features = Y_test_inv.shape[1]\n",
    "                fig, axes = plt.subplots(nrows=(num_features + 2)//3, ncols=3, figsize=(15, 5*((num_features + 2)//3)))\n",
    "                axes = axes.flatten()\n",
    "                titles = [\"Race progress\",\"Lap progress\",\"Fuel level\",\"Tyre wear FL\",\"Tyre wear FR\",\"Tyre wear RL\",\"Tyre wear RR\",\n",
    "                            \"Tyre temp FL\",\"Tyre temp FR\",\"Tyre temp RL\",\"Tyre temp RR\",\"Track wetness\"]\n",
    "                for i in range(num_features):\n",
    "                    ax = axes[i]\n",
    "                    ax.scatter(Y_test_inv[:, i], pred_test_inv[:, i], s=10, alpha=0.6)\n",
    "                    ax.plot([Y_test_inv[:, i].min(), Y_test_inv[:, i].max()],\n",
    "                            [Y_test_inv[:, i].min(), Y_test_inv[:, i].max()],\n",
    "                            'r--', linewidth=1.5, label='ideal line')\n",
    "                    ax.set_title(f'{titles[i]} | R²={r2s[i]:.3f}')\n",
    "                    ax.set_xlabel('True')\n",
    "                    ax.set_ylabel('Predicted')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True)\n",
    "\n",
    "                for j in range(i+1, len(axes)):\n",
    "                    fig.delaxes(axes[j])\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                num_features = Y_test_inv.shape[1]\n",
    "                titles = [\"Race progress\",\"Lap progress\",\"Fuel level\",\n",
    "                        \"Tyre wear FL\",\"Tyre wear FR\",\"Tyre wear RL\",\"Tyre wear RR\",\n",
    "                        \"Tyre temp FL\",\"Tyre temp FR\",\"Tyre temp RL\",\"Tyre temp RR\",\n",
    "                        \"Track wetness\"]\n",
    "\n",
    "                cols = 3\n",
    "                rows = int(np.ceil(num_features / cols))\n",
    "                fig, axes = plt.subplots(rows, cols, figsize=(18, rows * 3))\n",
    "                axes = axes.flatten()\n",
    "\n",
    "                for i in range(num_features):\n",
    "                    ax = axes[i]\n",
    "                    ax.plot(Y_test_inv[:, i], label='True', color='blue', linewidth=1)\n",
    "                    ax.plot(pred_test_inv[:, i], label='Pred', color='orange', linestyle='--', linewidth=1)\n",
    "                    ax.set_title(f'{titles[i]} | R²={r2s[i]:.3f}')\n",
    "                    ax.set_xlabel('Sample index')\n",
    "                    ax.set_ylabel('Value')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True)\n",
    "\n",
    "                # usuń puste osie, jeśli niepełny ostatni rząd\n",
    "                for j in range(i+1, len(axes)):\n",
    "                    fig.delaxes(axes[j])\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                                                \n",
    "    plt.figure()\n",
    "    plt.plot(fold_train_losses, label='Train')\n",
    "    plt.plot(fold_test_losses, label='Test')\n",
    "    plt.title(f'Fold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "del model, X_train_tensor, Y_train_tensor, X_test_tensor, Y_test_tensor\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "\n",
    "                # opcjonalnie: wróć do kształtu 3D, jeśli chcesz generować sekwencje\n",
    "                # pred_test_inv = pred_test_inv.reshape(b, t, f)\n",
    "                # Y_test_inv    = Y_test_inv.reshape(b, t, f)\n",
    "\n",
    "\n",
    "        \n",
    "    # all_fold_train.append(fold_train_losses)\n",
    "    # all_fold_test.append(fold_test_losses)\n",
    "    # # all_fold_r2.append(fold_r2_scores)\n",
    "\n",
    "    # # Compute R^2 score for the test set\n",
    "    # r2 = r2_score(Y_test_tensor.cpu().numpy(), pred_test.cpu().numpy())\n",
    "    # all_fold_r2_test.append(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_pred(sequence_x, window_size, n_steps_ahead=5):\n",
    "    X, Y = [], []\n",
    "    curr_len = len(sequence_x)\n",
    "\n",
    "    # for t in range(1, len(sequence_x)):\n",
    "    start = max(0, curr_len - window_size)\n",
    "    window = sequence_x[start:curr_len]\n",
    "\n",
    "    # padding na początku, jeśli okno krótsze niż window_size\n",
    "    pad_len = window_size - len(window)\n",
    "    if pad_len > 0:\n",
    "        window = np.vstack([np.zeros((pad_len, sequence_x.shape[1])), window])\n",
    "    X.append(window)\n",
    "\n",
    "        \n",
    "\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee06a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b913e63",
   "metadata": {},
   "source": [
    "Generate state predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_tensor, Y_train_tensor, X_test_tensor, Y_test_tensor\n",
    "window_size = 30\n",
    "X_init = X_test[0]  \n",
    "print(X_init)  # skalowane\n",
    "# current_window = torch.tensor(X_init, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "race_states = []\n",
    "race_states.append(X_init)\n",
    "current_window = create_window_pred(race_states,window_size=window_size)\n",
    "current_window = torch.tensor(current_window, dtype=torch.float32).to(device)\n",
    "model.eval()\n",
    "for _ in range(1000):\n",
    "    with torch.no_grad():\n",
    "        pred_test = model(current_window)\n",
    "        print(pred_test)\n",
    "        \n",
    "        race_states.append(pred_test.cpu().numpy())\n",
    "        current_window = create_window_pred(race_states, window_size=window_size)\n",
    "        current_window = torch.tensor(current_window, dtype=torch.float32).to(device)\n",
    "\n",
    "        pred_test_inv = scaler_Y.inverse_transform(pred_test.cpu().numpy())\n",
    "        Y_test_inv = scaler_Y.inverse_transform(current_window.squeeze(0).cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = y_cont_orig.shape[1]\n",
    "# plt.figure(figsize=(15, 3 * num_features))\n",
    "# titles = ['Lap progress','Race progress', 'Fuel level', 'Wheel1 wear', 'Wheel2 wear', 'Wheel3 wear', 'Wheel4 wear', 'Wheel1 temp','Wheel2 temp','Wheel3 temp','Wheel4 temp','AvgPathWetness']\n",
    "# for i in range(num_features):\n",
    "#     plt.subplot(num_features, 1, i + 1)\n",
    "#     plt.plot(y_cont_orig[:, i], label='Rzeczywiste')\n",
    "#     plt.plot(cont_pred_orig[:, i], label='Predykcja', linestyle='--')\n",
    "#     plt.title(f'{titles[i]}')\n",
    "#     plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39755fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, X_train_tensor, Y_train_tensor, X_test_tensor, Y_test_tensor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b3f6b",
   "metadata": {},
   "source": [
    "Visualize var to generate by alghoritm(weather,damages,itd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazwy cech według Twojego opisu\n",
    "feature_names = [\n",
    "    \"mLastImpactET\",\n",
    "    \"mLastImpactMagnitude\",\n",
    "    \"mNumPenalties\",\n",
    "    \"mRaining\",\n",
    "    \"mAmbientTemp\",\n",
    "    \"mTrackTemp\",\n",
    "    \"mDentSeverity[0]\",\n",
    "    \"mDentSeverity[1]\",\n",
    "    \"mDentSeverity[2]\",\n",
    "    \"mDentSeverity[3]\",\n",
    "    \"mDentSeverity[4]\",\n",
    "    \"mDentSeverity[5]\",\n",
    "    \"mDentSeverity[6]\",\n",
    "    \"mDentSeverity[7]\",\n",
    "    \"has_last_lap\",\n",
    "    \"mFinishStatus\",\n",
    "    \"mTotalLaps\",\n",
    "    \"mSector\",\n",
    "    \"mNumPitstops\",\n",
    "    \"mInPits\",\n",
    "    \"mFrontTireCompoundIndex\",\n",
    "    \"multiplier\"\n",
    "]\n",
    "\n",
    "# Jeśli masz mniej niż 24 nazw, dodaj puste lub dopisz brakujące\n",
    "while len(feature_names) < 24:\n",
    "    feature_names.append(f\"Feature {len(feature_names)}\")\n",
    "\n",
    "data = load_data_from_db()\n",
    "race = np.array(data[1])  # pierwszy wyścig\n",
    "last_24_features = race[:, -24:]  # shape: [liczba_kroków, 24]\n",
    "\n",
    "for i in range(24):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(last_24_features[:, i])\n",
    "    plt.title(f\"{feature_names[i]}\")\n",
    "    plt.xlabel(\"Krok\")\n",
    "    plt.ylabel(\"Wartość\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "data = load_data_from_db()\n",
    "num_features = 24\n",
    "\n",
    "# Inicjalizacja list na min i max dla każdej cechy\n",
    "feature_mins = [float('inf')] * num_features\n",
    "feature_maxs = [float('-inf')] * num_features\n",
    "\n",
    "for race in data:\n",
    "    race = np.array(race)\n",
    "    last_24_features = race[:, -num_features:]  # shape: [liczba_kroków, 24]\n",
    "    for i in range(num_features):\n",
    "        feature_min = np.min(last_24_features[:, i])\n",
    "        feature_max = np.max(last_24_features[:, i])\n",
    "        if feature_min < feature_mins[i]:\n",
    "            feature_mins[i] = feature_min\n",
    "        if feature_max > feature_maxs[i]:\n",
    "            feature_maxs[i] = feature_max\n",
    "\n",
    "for i in range(num_features):\n",
    "    print(f\"{feature_names[i]}: min={feature_mins[i]}, max={feature_maxs[i]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39b102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "race = np.array(data[0])  # pierwszy wyścig\n",
    "\n",
    "lap_distance = race[:, 0]  # podaj właściwy indeks dystansu okrążenia\n",
    "sector = race[:, 29]\n",
    "print(\"Unikalne sektory:\", np.unique(sector))\n",
    "\n",
    "for i in range(1, len(race)):\n",
    "    if sector[i] != sector[i-1]:\n",
    "        print(f\"Sektor zmienił się na {sector[i]} przy dystansie {lap_distance[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c861059",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_magnitudes = []\n",
    "\n",
    "for race in data:\n",
    "    race = np.array(race)\n",
    "    # Zakładam, że mLastImpactMagnitude to jedna z ostatnich 24 cech, np. indeks 1\n",
    "    impact_magnitudes.extend(race[:, -23])  # -23 jeśli to druga cecha z końca, popraw jeśli inny indeks\n",
    "\n",
    "impact_magnitudes = np.array(impact_magnitudes)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(impact_magnitudes, bins=90, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Rozkład siły uderzenia (mLastImpactMagnitude)\")\n",
    "plt.xlabel(\"Siła uderzenia\")\n",
    "plt.ylabel(\"Liczba przypadków\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zbierz wszystkie wartości siły uderzenia\n",
    "impact_magnitudes = []\n",
    "for race in data:\n",
    "    race = np.array(race)\n",
    "    impact_magnitudes.extend(race[:, -23])  # -23 jeśli to druga cecha z końca\n",
    "\n",
    "impact_magnitudes = np.array(impact_magnitudes)\n",
    "\n",
    "# Policz histogram (np. 20 przedziałów)\n",
    "hist, bin_edges = np.histogram(impact_magnitudes, bins=90, density=True)\n",
    "print(\"Bin edges:\", bin_edges)\n",
    "probabilities = hist / hist.sum()  # prawdopodobieństwa dla każdego przedziału\n",
    "print(probabilities)\n",
    "\n",
    "np.save('E:/pracadyp/Race-optimization-reinforcement-learning/data/probabilities_impact/probabilities.npy', probabilities)\n",
    "np.save('E:/pracadyp/Race-optimization-reinforcement-learning/data/probabilities_impact/bin_edges.npy', bin_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_from_db()\n",
    "num_features = 36\n",
    "\n",
    "# Inicjalizacja list na min i max dla każdej cechy\n",
    "feature_mins = [float('inf')] * num_features\n",
    "feature_maxs = [float('-inf')] * num_features\n",
    "\n",
    "for race in data:\n",
    "    race = np.array(race)\n",
    "    all_features = race[:, -num_features:]  # shape: [liczba_kroków, 36]\n",
    "    for i in range(num_features):\n",
    "        feature_min = np.min(all_features[:, i])\n",
    "        feature_max = np.max(all_features[:, i])\n",
    "        if feature_min < feature_mins[i]:\n",
    "            feature_mins[i] = feature_min\n",
    "        if feature_max > feature_maxs[i]:\n",
    "            feature_maxs[i] = feature_max\n",
    "\n",
    "for i in range(num_features):\n",
    "    print(f\": min={feature_mins[i]}, max={feature_maxs[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RsOPT_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
