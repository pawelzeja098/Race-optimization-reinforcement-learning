{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14876e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from envs.filtr_json_from_race import load_from_db\n",
    "import sqlite3\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "class LSTMStatePredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,dropout_prob=0.3):\n",
    "        super(LSTMStatePredictor, self).__init__()\n",
    "        \n",
    "        # Zapisz parametry (potrzebne do ewentualnego ręcznego\n",
    "        # tworzenia stanu, choć nie jest to już wymagane w forward)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        lstm_dropout_prob = dropout_prob if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=lstm_dropout_prob)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "    \n",
    "        # Twój świetny pomysł z wieloma głowicami - zostaje!\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, 2),  # progress\n",
    "            nn.Linear(hidden_size, 1),  # fuel\n",
    "            nn.Linear(hidden_size, 4),  # wear\n",
    "            nn.Linear(hidden_size, 4),  # temp\n",
    "            nn.Linear(hidden_size, 1)   # track wetness\n",
    "        ])\n",
    "\n",
    "        # Te pola nie są już potrzebne Wewnątrz modelu\n",
    "        # self.scaler_X = None\n",
    "        # self.scaler_Y = None\n",
    "        # self.n_steps_ahead = n_steps_ahead # Usuwamy n_steps_ahead\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x, h_c=None):\n",
    "        # 1. Nie musisz ręcznie inicjować h_c. \n",
    "        #    nn.LSTM zrobi to automatycznie, jeśli h_c jest None.\n",
    "        \n",
    "        # 2. Przetwórz CAŁĄ sekwencję\n",
    "        #    x ma kształt: [B, seq_len, 37]\n",
    "        #    out będzie miał kształt: [B, seq_len, hidden_size]\n",
    "        out, h_c = self.lstm(x, h_c) \n",
    "\n",
    "        out = self.dropout_layer(out)  # Zastosuj dropout do wyjścia LSTM\n",
    "        \n",
    "        # 3. Zastosuj głowice do CAŁEGO tensora 'out', a nie tylko 'out[:, -1, :]'\n",
    "        #    head(out) da np. [B, seq_len, 2]\n",
    "        outputs = [head(out) for head in self.heads]\n",
    "        \n",
    "        # 4. Połącz wzdłuż ostatniego wymiaru (wymiaru cech)\n",
    "        #    List of [B,S,2], [B,S,1], [B,S,4]... -> [B, S, 12]\n",
    "        #    Używamy dim=2, ponieważ kształt to (Batch, Seq_len, Features)\n",
    "        combined = torch.cat(outputs, dim=2) \n",
    "        \n",
    "        return combined, h_c\n",
    "\n",
    "def create_scalers(X,Y):\n",
    "\n",
    "    cont_indices_x = slice(0, 19)   # continuous columns for X (0–18)\n",
    "    cont_indices_y = slice(0, 12)   # continuous columns for Y (0–11)\n",
    "\n",
    "    # Scale continuous features\n",
    "    flat_x = np.vstack([x[:, cont_indices_x] for x in X])\n",
    "    flat_y = np.vstack([y[:, cont_indices_y] for y in Y])\n",
    "\n",
    "    # scaler_X = MinMaxScaler().fit(flat_x)\n",
    "    # # scaler_Y = MinMaxScaler().fit(flat_y)\n",
    "    # scaler_Y = StandardScaler().fit(flat_y)\n",
    "    scaler_X = RobustScaler().fit(flat_x)\n",
    "    scaler_Y = RobustScaler().fit(flat_y)\n",
    "    return scaler_X, scaler_Y\n",
    "\n",
    "def scale_input(X, Y, scaler_X, scaler_Y):\n",
    "    cont_indices_x = slice(0, 19)   # continuous columns for X\n",
    "    cont_indices_y = slice(0, 12)   # continuous columns for Y\n",
    "\n",
    "    X_scaled_grouped = []\n",
    "    Y_scaled_grouped = []\n",
    "\n",
    "    for x_seq, y_seq in zip(X, Y):\n",
    "        x_scaled = np.array(x_seq, dtype=float)\n",
    "        x_scaled[:, cont_indices_x] = scaler_X.transform(x_seq[:, cont_indices_x])\n",
    "        X_scaled_grouped.append(x_scaled)\n",
    "\n",
    "        y_scaled = np.array(y_seq, dtype=float)\n",
    "        y_scaled[:, cont_indices_y] = scaler_Y.transform(y_seq[:, cont_indices_y])\n",
    "        Y_scaled_grouped.append(y_scaled)\n",
    "\n",
    "    # Conversion to torch tensors\n",
    "    # X_t = [torch.tensor(x, dtype=torch.float32) for x in X_scaled_grouped]\n",
    "    # Y_cont_t = [torch.tensor(y[:, cont_indices_y], dtype=torch.float32) for y in Y_scaled_grouped]\n",
    "\n",
    "    return X_scaled_grouped, Y_scaled_grouped\n",
    "\n",
    "# def scale_single_input(x, scaler_X):\n",
    "#     cont_indices_x = slice(0, 19)   # continuous columns for X\n",
    "#     X_scaled_grouped = []\n",
    "\n",
    "#     for x_seq in x:\n",
    "#         x_scaled = np.array(x_seq, dtype=float)\n",
    "#         x_scaled[:, cont_indices_x] = scaler_X.transform(x_seq[:, cont_indices_x])\n",
    "#         X_scaled_grouped.append(x_scaled)\n",
    "#     return X_scaled_grouped\n",
    "\n",
    "def scale_single_input(raw_vector_x, scaler_x_cont):\n",
    "    \"\"\"\n",
    "    Skaluje pojedynczy wektor (37,), stosując scaler tylko do \n",
    "    części ciągłej (0-19) i zostawiając kategorialną (20-36).\n",
    "    \"\"\"\n",
    "    cont_indices_x = slice(0, 19)\n",
    "    cat_indices_x = slice(19, 38)\n",
    "    \n",
    "    # raw_vector_x[cont_indices_x] ma kształt (19,)\n",
    "    # Musimy go przekształcić na (1, 19) dla scalera\n",
    "    x_cont_scaled = scaler_x_cont.transform([raw_vector_x[cont_indices_x]])\n",
    "    \n",
    "    # raw_vector_x[cat_indices_x] ma kształt (18,)\n",
    "    # --- POPRAWKA TUTAJ ---\n",
    "    # Musimy go przekształcić na (1, 18), aby pasował do hstack\n",
    "    x_cat = raw_vector_x[cat_indices_x].reshape(1, -1)\n",
    "    \n",
    "    # Teraz łączymy (1, 19) z (1, 18) -> (1, 37)\n",
    "    # i spłaszczamy z powrotem do 1D (37,)\n",
    "    return np.hstack([x_cont_scaled, x_cat]).flatten()\n",
    "       \n",
    "\n",
    "def create_window_pred(sequence_x, window_size, n_steps_ahead=5):\n",
    "    X, Y = [], []\n",
    "    sequence_x = np.array(sequence_x)\n",
    "    curr_len = len(sequence_x)\n",
    "\n",
    "    start = max(0, curr_len - window_size)\n",
    "    window = sequence_x[start:curr_len]\n",
    "\n",
    "    pad_len = window_size - len(window)\n",
    "    if pad_len > 0:\n",
    "        window = np.vstack([np.zeros((pad_len, sequence_x.shape[1])), window])\n",
    "\n",
    "    # dodaj batch dimension\n",
    "    X = window[np.newaxis, :, :]  # shape [1, window_size, num_features]\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def generate_predictions(model, input_seq,scaler_X=None, scaler_Y=None,h_c=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    lap_dist_sin = np.sin(2 * np.pi * input_seq[0])\n",
    "\n",
    "    lap_dist_cos = np.cos(2 * np.pi * input_seq[0])\n",
    "    # input_seq = create_window_pred(input_seq, window_size=30, n_steps_ahead=n_steps_ahead)\n",
    "    input_seq = np.hstack([\n",
    "        lap_dist_sin, \n",
    "        lap_dist_cos, \n",
    "        input_seq[1:]  # <-- Pomijamy starą cechę LAP_DIST\n",
    "    ])\n",
    "    input_seq = scale_single_input(input_seq, scaler_X)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "         \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).reshape(1, 1, 38).to(device)\n",
    "        predictions , h_c = model(input_tensor, h_c)\n",
    "        predictions = predictions.cpu().numpy().reshape(1, 12)\n",
    "        \n",
    "        predictions = scaler_Y.inverse_transform(predictions)\n",
    "        return predictions.flatten(), h_c\n",
    "    \n",
    "def load_data_from_db():\n",
    "    \n",
    "    \"\"\"\n",
    "    Load data so that each race is a separate sequence:\n",
    "    X = [ [state1_race1, state2_race1, ...], [state1_race2, ...] ]\n",
    "    Y = [ [next1_race1, next2_race1, ...], ... ]\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(\n",
    "        \"E:/pracadyp/Race-optimization-reinforcement-learning/data/db_states_for_regress/race_data_states.db\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT race_id, states_json FROM races ORDER BY race_id\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for race_id, states_json in rows:\n",
    "        states = json.loads(states_json)\n",
    "        data.append(states)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_x_y(data):\n",
    "    X_grouped, Y_grouped = [], []\n",
    "\n",
    "    for race in data:\n",
    "        X_seq, Y_seq = [], []\n",
    "        for i in range(len(race) - 1):\n",
    "            X_seq.append(race[i][:-2])\n",
    "            Y_seq.append(race[i + 1][:-28]) \n",
    "        \n",
    "        # dodajemy każdy wyścig osobno\n",
    "        X_grouped.append(np.array(X_seq, dtype=float))\n",
    "        Y_grouped.append(np.array(Y_seq, dtype=float))\n",
    "\n",
    "    return X_grouped, Y_grouped\n",
    "\n",
    "def create_windows(sequence_x, sequence_y, window_size, n_steps_ahead=5):\n",
    "    X, Y = [], []\n",
    "    for t in range(1, len(sequence_x)):\n",
    "        start = max(0, t - window_size)\n",
    "        window = sequence_x[start:t]\n",
    "\n",
    "        # padding na początku, jeśli okno krótsze niż window_size\n",
    "        pad_len = window_size - len(window)\n",
    "        if pad_len > 0:\n",
    "            window = np.vstack([np.zeros((pad_len, sequence_x.shape[1])), window])\n",
    "        X.append(window)\n",
    "\n",
    "        # Y: wypełniamy zerami, jeśli końcówka wyścigu ma mniej niż n_steps_ahead\n",
    "        y_window = sequence_y[t:t+n_steps_ahead]\n",
    "        if y_window.shape[0] < n_steps_ahead:\n",
    "            pad = np.zeros((n_steps_ahead - y_window.shape[0], sequence_y.shape[1]))\n",
    "            y_window = np.vstack([y_window, pad])\n",
    "        Y.append(y_window)\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def create_sliding_windows(races_x_list, races_y_list, sequence_length, step=1):\n",
    "    \"\"\"\n",
    "    Tworzy próbki (X, Y) metodą przesuwnego okna dla Teacher Forcing.\n",
    "    X = [t, t+1, ..., t+seq_len-1]\n",
    "    Y = [t+1, t+2, ..., t+seq_len]  (przesunięte o 1)\n",
    "    \"\"\"\n",
    "    all_X_samples = []\n",
    "    all_Y_samples = []\n",
    "    \n",
    "    # Pamiętaj, że race_y to już wyodrębnione 12 cech\n",
    "    # race_x to pełne 37 cech\n",
    "    \n",
    "    for race_x, race_y in zip(races_x_list, races_y_list):\n",
    "        race_length = race_x.shape[0]\n",
    "        \n",
    "        # Pętla po pojedynczym wyścigu\n",
    "        # Ostatni indeks startowy `i` musi być taki, aby `i + sequence_length`\n",
    "        # nie wyszło poza zakres dla Y (który jest przesunięty o 1)\n",
    "        for i in range(0, race_length - sequence_length, step):\n",
    "            \n",
    "            # X: Kształt (sequence_length, 37)\n",
    "            x_sample = race_x[i : i + sequence_length]\n",
    "            \n",
    "            # Y: Kształt (sequence_length, 12)\n",
    "            # Dla wejścia X w kroku 't', celem jest Y z kroku 't+1'\n",
    "            y_sample = race_y[i + 1 : i + sequence_length + 1] \n",
    "            \n",
    "            all_X_samples.append(x_sample)\n",
    "            all_Y_samples.append(y_sample)\n",
    "            \n",
    "    return np.array(all_X_samples), np.array(all_Y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db52ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_data_from_db()\n",
    "    \n",
    "   \n",
    "# X, Y = create_x_y(data)\n",
    "# input_size = X[0].shape[1]\n",
    "# output_size = Y[0].shape[1]\n",
    "# print(input_size, output_size)\n",
    "\n",
    "# SEQUENCE_LENGTH = 800\n",
    "# STEP = 1\n",
    "\n",
    "# lr = 1e-4\n",
    "# batch_size = 128\n",
    "# num_epochs = 40\n",
    "# weight = [0.5, 1.8, 3.0, 0.1, 1.5]\n",
    "\n",
    "\n",
    "# scaler_X, scaler_Y = create_scalers(X,Y)\n",
    "\n",
    "# X_train, Y_train = scale_input(X,Y,scaler_X,scaler_Y)\n",
    "\n",
    "# # n_steps_ahead = 5  # number of future steps to predict\n",
    "\n",
    "\n",
    "# # all_X, all_Y = [], []\n",
    "# # for race_x, race_y in zip(X_train, Y_train):  \n",
    "# #     X_r, Y_r = create_windows(race_x, race_y, window_size=30, n_steps_ahead=n_steps_ahead)\n",
    "# #     all_X.append(X_r)\n",
    "# #     all_Y.append(Y_r)\n",
    "\n",
    "# print(\"Tworzenie sampli treningowych...\")\n",
    "# X_train_samples, Y_train_samples = create_sliding_windows(\n",
    "#     X_train, Y_train, SEQUENCE_LENGTH, STEP\n",
    "# )\n",
    "\n",
    "# # X_train = np.vstack(all_X)  # shape: [N_samples, window_size, n_features]\n",
    "# # Y_train = np.vstack(all_Y) \n",
    "# # all_X, all_Y = [], []\n",
    "# # for race_x, race_y in zip(X_test, Y_test):  \n",
    "# #     X_r, Y_r = create_windows(race_x, race_y, window_size=30)\n",
    "# #     all_X.append(X_r)\n",
    "# #     all_Y.append(Y_r)\n",
    "# # X_test = np.vstack(all_X)  # shape: [N_samples, window_size, n_features]\n",
    "# # Y_test = np.vstack(all_Y)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "# model = LSTMStatePredictor(input_size=input_size, hidden_size=256, output_size=output_size, num_layers=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_samples, dtype=torch.float32).to(device)\n",
    "# Y_train_tensor = torch.tensor(Y_train_samples, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "# loss_cont = nn.MSELoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "    \n",
    "#     model.train()\n",
    "#     total_train_loss = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Model dostaje całą sekwencję 200 kroków\n",
    "#         # i zwraca predykcje dla całej sekwencji 200 kroków\n",
    "#         y_pred, _ = model(x_batch) \n",
    "        \n",
    "#         # y_pred ma kształt (batch_size, SEQUENCE_LENGTH, 12)\n",
    "        \n",
    "#         # Obliczamy stratę dla całej sekwencji na raz\n",
    "#         # Musimy indeksować wymiar cech [:, :, ...]\n",
    "#         loss_progress = loss_cont(y_pred[:, :, 0:2], y_batch[:, :, 0:2])\n",
    "#         loss_fuel     = loss_cont(y_pred[:, :, 2:3], y_batch[:, :, 2:3])\n",
    "#         loss_wear     = loss_cont(y_pred[:, :, 3:7], y_batch[:, :, 3:7])\n",
    "#         loss_temp     = loss_cont(y_pred[:, :, 7:11], y_batch[:, :, 7:11])\n",
    "#         loss_wet      = loss_cont(y_pred[:, :, 11:], y_batch[:, :, 11:])\n",
    "        \n",
    "#         # Sumujemy straty (tak jak miałeś)\n",
    "#         loss = (weight[0] * loss_progress + \n",
    "#                 weight[1] * loss_fuel + \n",
    "#                 weight[2] * loss_wear + \n",
    "#                 weight[3] * loss_temp + \n",
    "#                 weight[4] * loss_wet)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_train_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "#     scheduler.step(avg_train_loss)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c76a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używane urządzenie: cuda\n",
      "--- Rozpoczynam Eksperyment K-Fold (K=5) ---\n",
      "\n",
      "--- FOLD 1/5 ---\n",
      "Tworzenie sampli treningowych...\n",
      "Tworzenie sampli walidacyjnych...\n",
      "  Epoch 5/100, Train Loss: 0.1022, Val Loss: 0.1229\n",
      "  Epoch 10/100, Train Loss: 0.0814, Val Loss: 0.0785\n",
      "  Epoch 15/100, Train Loss: 0.0747, Val Loss: 0.0511\n",
      "  Epoch 20/100, Train Loss: 0.0707, Val Loss: 0.0378\n",
      "  Epoch 25/100, Train Loss: 0.0677, Val Loss: 0.0300\n",
      "  Epoch 30/100, Train Loss: 0.0657, Val Loss: 0.0260\n",
      "  Epoch 35/100, Train Loss: 0.0646, Val Loss: 0.0209\n",
      "  Epoch 40/100, Train Loss: 0.0635, Val Loss: 0.0196\n",
      "  Epoch 45/100, Train Loss: 0.0630, Val Loss: 0.0170\n",
      "  Epoch 50/100, Train Loss: 0.0624, Val Loss: 0.0165\n",
      "  Epoch 55/100, Train Loss: 0.0621, Val Loss: 0.0158\n",
      "  Epoch 60/100, Train Loss: 0.0618, Val Loss: 0.0144\n",
      "  Epoch 65/100, Train Loss: 0.0617, Val Loss: 0.0136\n",
      "  Epoch 70/100, Train Loss: 0.0617, Val Loss: 0.0123\n",
      "  Epoch 75/100, Train Loss: 0.0614, Val Loss: 0.0120\n",
      "  Epoch 80/100, Train Loss: 0.0612, Val Loss: 0.0122\n",
      "  Epoch 85/100, Train Loss: 0.0612, Val Loss: 0.0112\n",
      "  Epoch 90/100, Train Loss: 0.0612, Val Loss: 0.0109\n",
      "  Epoch 95/100, Train Loss: 0.0611, Val Loss: 0.0111\n",
      "  Epoch 100/100, Train Loss: 0.0609, Val Loss: 0.0107\n",
      "Fold 1: Najlepszy wynik w epoce 89 (Val Loss: 0.0104)\n",
      "\n",
      "--- FOLD 2/5 ---\n",
      "Tworzenie sampli treningowych...\n",
      "Tworzenie sampli walidacyjnych...\n"
     ]
    }
   ],
   "source": [
    "data = load_data_from_db()\n",
    "X, Y = create_x_y(data) # X, Y to LISTY wyścigów\n",
    "\n",
    "# === 2. Hiperparametry Eksperymentu ===\n",
    "# Upewnij się, że znasz najkrótszy wyścig (np. 1650)\n",
    "SEQUENCE_LENGTH = 1000 # Musi być > 1 okrążenia (np. 833) i < najkrótszego wyścigu\n",
    "STEP = 1\n",
    "lr = 1e-4\n",
    "# Zmniejszyłem batch_size, ponieważ seq_length=1000 zużyje znacznie więcej VRAM\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 100       # Wystarczająco dużo, aby EarlyStopping zadziałał\n",
    "PATIENCE = 15          # Cierpliwość dla EarlyStopping\n",
    "weight = [0.5, 1.8, 3.0, 0.1, 1.5]\n",
    "DROPOUT_RATE = 0.4     # Z naszej poprzedniej rozmowy\n",
    "\n",
    "# === 3. Krytyczne Ustawienia Modelu (Na podstawie naszych rozmów) ===\n",
    "# Zakładam, że Twoje wejście X ma 38 cech (z sin/cos I RaceDIST)\n",
    "# Zakładam, że Twoje wyjście Y ma 12 cech (z sin/cos, BEZ RaceDIST)\n",
    "input_size = 38 \n",
    "output_size = 12 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "loss_cont = nn.MSELoss()\n",
    "\n",
    "# === 4. Pętla Walidacji Krzyżowej (K-Fold) ===\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=12343)\n",
    "all_best_epochs = [] # Tu zbieramy wyniki\n",
    "\n",
    "print(f\"--- Rozpoczynam Eksperyment K-Fold (K={N_SPLITS}) ---\")\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # 1. Podział danych na listy wyścigów\n",
    "    X_train_races = [X[i] for i in train_idx]\n",
    "    Y_train_races = [Y[i] for i in train_idx]\n",
    "    X_val_races = [X[i] for i in val_idx]\n",
    "    Y_val_races = [Y[i] for i in val_idx]\n",
    "\n",
    "    # 2. Tworzenie skalerów (TYLKO na danych treningowych tego foldu)\n",
    "    scaler_X, scaler_Y = create_scalers(X_train_races, Y_train_races)\n",
    "\n",
    "    # 3. Skalowanie obu zbiorów\n",
    "    X_train_scaled, Y_train_scaled = scale_input(X_train_races, Y_train_races, scaler_X, scaler_Y)\n",
    "    X_val_scaled, Y_val_scaled     = scale_input(X_val_races, Y_val_races, scaler_X, scaler_Y)\n",
    "\n",
    "    # 4. Tworzenie okien (sampli)\n",
    "    print(\"Tworzenie sampli treningowych...\")\n",
    "    X_train_samples, Y_train_samples = create_sliding_windows(X_train_scaled, Y_train_scaled, SEQUENCE_LENGTH, STEP)\n",
    "    print(\"Tworzenie sampli walidacyjnych...\")\n",
    "    X_val_samples, Y_val_samples = create_sliding_windows(X_val_scaled, Y_val_scaled, SEQUENCE_LENGTH, STEP)\n",
    "    \n",
    "    if len(X_val_samples) == 0:\n",
    "        print(f\"BŁĄD: Fold {fold+1} nie ma danych walidacyjnych. Prawdopodobnie seq_length jest za duży.\")\n",
    "        continue\n",
    "\n",
    "    # 5. Tworzenie Tensorów i DataLoaderów\n",
    "    X_train_tensor = torch.tensor(X_train_samples, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train_samples, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val_samples, dtype=torch.float32)\n",
    "    Y_val_tensor = torch.tensor(Y_val_samples, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 6. Inicjalizacja modelu i optimizera (OD NOWA dla każdego foldu)\n",
    "    model = LSTMStatePredictor(\n",
    "        input_size=input_size, \n",
    "        hidden_size=256, \n",
    "        output_size=output_size, \n",
    "        num_layers=1,\n",
    "        dropout_prob=DROPOUT_RATE\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "\n",
    "    # 7. Zmienne do EarlyStopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_this_fold = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # === 5. Wewnętrzna pętla treningu (Epoch) ===\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, _ = model(x_batch.to(device))\n",
    "            \n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            loss_progress = loss_cont(y_pred[:, :, 0:2], y_batch[:, :, 0:2])\n",
    "            loss_fuel     = loss_cont(y_pred[:, :, 2:3], y_batch[:, :, 2:3])\n",
    "            loss_wear     = loss_cont(y_pred[:, :, 3:7], y_batch[:, :, 3:7])\n",
    "            loss_temp     = loss_cont(y_pred[:, :, 7:11], y_batch[:, :, 7:11])\n",
    "            loss_wet      = loss_cont(y_pred[:, :, 11:], y_batch[:, :, 11:])\n",
    "            \n",
    "            # Sumujemy straty (tak jak miałeś)\n",
    "            loss = (weight[0] * loss_progress + \n",
    "                    weight[1] * loss_fuel + \n",
    "                    weight[2] * loss_wear + \n",
    "                    weight[3] * loss_temp + \n",
    "                    weight[4] * loss_wet)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # === 6. Pętla Walidacyjna (NOWA) ===\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                y_pred, _ = model(x_batch.to(device))\n",
    "\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                loss_progress = loss_cont(y_pred[:, :, 0:2], y_batch[:, :, 0:2])\n",
    "                loss_fuel     = loss_cont(y_pred[:, :, 2:3], y_batch[:, :, 2:3])\n",
    "                loss_wear     = loss_cont(y_pred[:, :, 3:7], y_batch[:, :, 3:7])\n",
    "                loss_temp     = loss_cont(y_pred[:, :, 7:11], y_batch[:, :, 7:11])\n",
    "                loss_wet      = loss_cont(y_pred[:, :, 11:], y_batch[:, :, 11:])\n",
    "                \n",
    "                # Sumujemy straty (tak jak miałeś)\n",
    "                val_loss = (weight[0] * loss_progress + \n",
    "                        weight[1] * loss_fuel + \n",
    "                        weight[2] * loss_wear + \n",
    "                        weight[3] * loss_temp + \n",
    "                        weight[4] * loss_wet)\n",
    "                total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        # === 7. Logika Schedulera i EarlyStopping ===\n",
    "        scheduler.step(avg_val_loss) # <-- Użyj avg_val_loss\n",
    "\n",
    "        if (epoch + 1) % 5 == 0: # Drukuj co 5 epok\n",
    "            print(f\"  Epoch {epoch+1}/{MAX_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch_this_fold = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"  Early stopping w epoce {epoch + 1}.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Fold {fold + 1}: Najlepszy wynik w epoce {best_epoch_this_fold} (Val Loss: {best_val_loss:.4f})\")\n",
    "    all_best_epochs.append(best_epoch_this_fold)\n",
    "\n",
    "# === 8. Obliczenie Mediany (Po wszystkich foldach) ===\n",
    "if all_best_epochs:\n",
    "    FINAL_EPOCH_NUMBER = int(np.median(all_best_epochs))\n",
    "    print(f\"\\n--- Eksperyment Zakończony ---\")\n",
    "    print(f\"Najlepsze epoki z każdego foldu: {all_best_epochs}\")\n",
    "    print(f\"==> Mediana optymalnych epok (Twoja 'Złota Liczba'): {FINAL_EPOCH_NUMBER}\")\n",
    "else:\n",
    "    print(\"\\n--- Eksperyment NIE POWIÓDŁ SIĘ ---\")\n",
    "    print(\"Nie zebrano żadnych wyników. Sprawdź, czy 'create_sliding_windows' zwraca dane walidacyjne.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RsOPT_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
