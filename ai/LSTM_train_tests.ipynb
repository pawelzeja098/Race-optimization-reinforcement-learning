{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14876e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from envs.filtr_json_from_race import load_from_db\n",
    "import sqlite3\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from config import X_SHAPE, Y_SHAPE, CONT_LENGTH, CAT_LENGTH\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db157dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import X_SHAPE, Y_SHAPE, CONT_LENGTH, CAT_LENGTH, Y_INDEXES, NO_SCALER_IDXES_X, MIN_MAX_SCALER_IDXES_X, ROBUST_SCALER_IDXES_X, NO_SCALER_IDEXES_Y, MIN_MAX_SCALER_IDXES_Y, ROBUST_SCALER_IDXES_Y\n",
    "\n",
    "\n",
    "\n",
    "class LSTMStatePredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,dropout_prob=0.3):\n",
    "        super(LSTMStatePredictor, self).__init__()\n",
    "        \n",
    "        # Zapisz parametry (potrzebne do ewentualnego ręcznego\n",
    "        # tworzenia stanu, choć nie jest to już wymagane w forward)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        lstm_dropout_prob = dropout_prob if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=lstm_dropout_prob)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "        self.act_delta = nn.ReLU()\n",
    "    \n",
    "      \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, 2),  # progress\n",
    "            nn.Linear(hidden_size, 1),  # fuel\n",
    "            nn.Linear(hidden_size, 4),  # wear\n",
    "            nn.Linear(hidden_size, 4) # temp\n",
    "            # nn.Linear(hidden_size, 1)   # track wetness\n",
    "        ])\n",
    "\n",
    "      \n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x, h_c=None):\n",
    "        # 1. Nie musisz ręcznie inicjować h_c. \n",
    "        #    nn.LSTM zrobi to automatycznie, jeśli h_c jest None.\n",
    "        \n",
    "        # 2. Przetwórz CAŁĄ sekwencję\n",
    "        #    x ma kształt: [B, seq_len, 37]\n",
    "        #    out będzie miał kształt: [B, seq_len, hidden_size]\n",
    "        out, h_c = self.lstm(x, h_c) \n",
    "\n",
    "        out = self.dropout_layer(out)  # Zastosuj dropout do wyjścia LSTM\n",
    "        \n",
    "        # 3. Zastosuj głowice do CAŁEGO tensora 'out', a nie tylko 'out[:, -1, :]'\n",
    "        #    head(out) da np. [B, seq_len, 2]\n",
    "        outputs = [head(out) for head in self.heads]\n",
    "\n",
    "        outputs[2] = self.act_delta(outputs[2])\n",
    "\n",
    "\n",
    "        \n",
    "        # 4. Połącz wzdłuż ostatniego wymiaru (wymiaru cech)\n",
    "        #    List of [B,S,2], [B,S,1], [B,S,4]... -> [B, S, 12]\n",
    "        #    Używamy dim=2, ponieważ kształt to (Batch, Seq_len, Features)\n",
    "        combined = torch.cat(outputs, dim=2) \n",
    "        \n",
    "        return combined, h_c\n",
    "\n",
    "def create_scalers(X,Y):\n",
    "\n",
    "    # cont_indices_x = slice(0, CONT_LENGTH)   # continuous columns for X (0–18)\n",
    "    # cont_indices_y = slice(0, Y_SHAPE)   # continuous columns for Y (0–11)\n",
    "\n",
    "    no_scaler_x = slice(0, NO_SCALER_IDXES_X)  # no scaler for X\n",
    "    min_max_scaler_x = slice(NO_SCALER_IDXES_X, MIN_MAX_SCALER_IDXES_X)  # min-max scaler for X\n",
    "    robust_scaler_x = slice(MIN_MAX_SCALER_IDXES_X, ROBUST_SCALER_IDXES_X)  # robust scaler for X\n",
    "    no_scaler_y = slice(0, NO_SCALER_IDEXES_Y)  # no scaler for Y\n",
    "    min_max_scaler_y = slice(NO_SCALER_IDEXES_Y, MIN_MAX_SCALER_IDXES_Y)  # min-max scaler for Y\n",
    "    robust_scaler_y = slice(MIN_MAX_SCALER_IDXES_Y, ROBUST_SCALER_IDXES_Y)  # robust scaler for Y\n",
    "\n",
    "   \n",
    "    flat_x_min_max = np.vstack([x[:, min_max_scaler_x] for x in X])\n",
    "    flat_x_robust = np.vstack([x[:, robust_scaler_x] for x in X])\n",
    "    flat_y_min_max = np.vstack([y[:, min_max_scaler_y] for y in Y])\n",
    "    flat_y_robust = np.vstack([y[:, robust_scaler_y] for y in Y])\n",
    "\n",
    "    scaler_X_min_max = MinMaxScaler().fit(flat_x_min_max)\n",
    "    scaler_X_robust = RobustScaler().fit(flat_x_robust)\n",
    "    scaler_Y_min_max = MinMaxScaler().fit(flat_y_min_max)\n",
    "    scaler_Y_robust = RobustScaler().fit(flat_y_robust)\n",
    "\n",
    "   \n",
    "    return scaler_X_min_max, scaler_X_robust, scaler_Y_min_max, scaler_Y_robust\n",
    "\n",
    "def scale_input(X, Y, scaler_X_min_max, scaler_X_robust, scaler_Y_min_max, scaler_Y_robust):\n",
    "    no_scaler_x = slice(0, NO_SCALER_IDXES_X)  # no scaler for X\n",
    "    min_max_scaler_x = slice(NO_SCALER_IDXES_X, MIN_MAX_SCALER_IDXES_X)  # min-max scaler for X\n",
    "    robust_scaler_x = slice(MIN_MAX_SCALER_IDXES_X, ROBUST_SCALER_IDXES_X)  # robust scaler for X\n",
    "    no_scaler_y = slice(0, NO_SCALER_IDEXES_Y)  # no scaler for Y\n",
    "    min_max_scaler_y = slice(NO_SCALER_IDEXES_Y, MIN_MAX_SCALER_IDXES_Y)  # min-max scaler for Y\n",
    "    robust_scaler_y = slice(MIN_MAX_SCALER_IDXES_Y, ROBUST_SCALER_IDXES_Y)  # robust scaler for Y\n",
    "\n",
    "    X_scaled_grouped = []\n",
    "    Y_scaled_grouped = []\n",
    "\n",
    "    for x_seq, y_seq in zip(X, Y):\n",
    "        x_scaled = np.array(x_seq, dtype=float)\n",
    "        x_scaled[:, min_max_scaler_x] = scaler_X_min_max.transform(x_seq[:, min_max_scaler_x])\n",
    "        x_scaled[:, robust_scaler_x] = scaler_X_robust.transform(x_seq[:, robust_scaler_x])\n",
    "        X_scaled_grouped.append(x_scaled)\n",
    "\n",
    "        y_scaled = np.array(y_seq, dtype=float)\n",
    "        y_scaled[:, min_max_scaler_y] = scaler_Y_min_max.transform(y_seq[:, min_max_scaler_y])\n",
    "        y_scaled[:, robust_scaler_y] = scaler_Y_robust.transform(y_seq[:, robust_scaler_y])\n",
    "        Y_scaled_grouped.append(y_scaled)\n",
    "\n",
    "    # Conversion to torch tensors\n",
    "    # X_t = [torch.tensor(x, dtype=torch.float32) for x in X_scaled_grouped]\n",
    "    # Y_cont_t = [torch.tensor(y[:, cont_indices_y], dtype=torch.float32) for y in Y_scaled_grouped]\n",
    "\n",
    "    return X_scaled_grouped, Y_scaled_grouped\n",
    "\n",
    "# def scale_single_input(x, scaler_X):\n",
    "#     cont_indices_x = slice(0, 19)   # continuous columns for X\n",
    "#     X_scaled_grouped = []\n",
    "\n",
    "#     for x_seq in x:\n",
    "#         x_scaled = np.array(x_seq, dtype=float)\n",
    "#         x_scaled[:, cont_indices_x] = scaler_X.transform(x_seq[:, cont_indices_x])\n",
    "#         X_scaled_grouped.append(x_scaled)\n",
    "#     return X_scaled_grouped\n",
    "\n",
    "def scale_single_input(raw_vector_x, scaler_X_min_max, scaler_X_robust):\n",
    "    \"\"\"\n",
    "    Skaluje pojedynczy wektor (37,), stosując scaler tylko do \n",
    "    części ciągłej (0-19) i zostawiając kategorialną (20-36).\n",
    "    \"\"\"\n",
    "    no_scaler_x = slice(0, NO_SCALER_IDXES_X)  # no scaler for X\n",
    "    min_max_scaler_x = slice(NO_SCALER_IDXES_X, MIN_MAX_SCALER_IDXES_X)  # min-max scaler for X\n",
    "    robust_scaler_x = slice(MIN_MAX_SCALER_IDXES_X, ROBUST_SCALER_IDXES_X)  # robust scaler for X\n",
    " \n",
    "    \n",
    "    # raw_vector_x[cont_indices_x] ma kształt (19,)\n",
    "    # Musimy go przekształcić na (1, 19) dla scalera\n",
    "    x_min_max_scaled = scaler_X_min_max.transform([raw_vector_x[min_max_scaler_x]])\n",
    "    x_robust_scaled = scaler_X_robust.transform([raw_vector_x[robust_scaler_x]])\n",
    "    \n",
    "    # raw_vector_x[cat_indices_x] ma kształt (18,)\n",
    "    # --- POPRAWKA TUTAJ ---\n",
    "    # Musimy go przekształcić na (1, 19), aby pasował do hstack\n",
    "    x_no_scaled = raw_vector_x[no_scaler_x].reshape(1, -1)\n",
    "    \n",
    "    # Teraz łączymy (1, 19) z (1, 18) -> (1, 37)\n",
    "    # i spłaszczamy z powrotem do 1D (37,)\n",
    "    return np.hstack([x_no_scaled, x_min_max_scaled, x_robust_scaled]).flatten()\n",
    "       \n",
    "\n",
    "# def create_window_pred(sequence_x, window_size, n_steps_ahead=5):\n",
    "#     X, Y = [], []\n",
    "#     sequence_x = np.array(sequence_x)\n",
    "#     curr_len = len(sequence_x)\n",
    "\n",
    "#     start = max(0, curr_len - window_size)\n",
    "#     window = sequence_x[start:curr_len]\n",
    "\n",
    "#     pad_len = window_size - len(window)\n",
    "#     if pad_len > 0:\n",
    "#         window = np.vstack([np.zeros((pad_len, sequence_x.shape[1])), window])\n",
    "\n",
    "#     # dodaj batch dimension\n",
    "#     X = window[np.newaxis, :, :]  # shape [1, window_size, num_features]\n",
    "#     return X\n",
    "\n",
    "\n",
    "\n",
    "def generate_predictions(model, input_seq,scaler_X_min_max=None, scaler_X_robust=None, scaler_Y_min_max=None, scaler_Y_robust=None,h_c=None):\n",
    "    no_scaler_y = slice(0, NO_SCALER_IDEXES_Y)  # no scaler for Y\n",
    "    min_max_scaler_y = slice(NO_SCALER_IDEXES_Y, MIN_MAX_SCALER_IDXES_Y)  # min-max scaler for Y\n",
    "    robust_scaler_y = slice(MIN_MAX_SCALER_IDXES_Y, ROBUST_SCALER_IDXES_Y)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    lap_dist_sin = np.sin(2 * np.pi * input_seq[0])\n",
    "\n",
    "    lap_dist_cos = np.cos(2 * np.pi * input_seq[0])\n",
    "    # input_seq = create_window_pred(input_seq, window_size=30, n_steps_ahead=n_steps_ahead)\n",
    "    input_seq = np.hstack([\n",
    "        lap_dist_sin, \n",
    "        lap_dist_cos, \n",
    "        input_seq[1:]  # <-- Pomijamy starą cechę LAP_DIST\n",
    "    ])\n",
    "    input_seq = scale_single_input(input_seq, scaler_X_min_max, scaler_X_robust)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "         \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).reshape(1, 1, X_SHAPE).to(device)\n",
    "        predictions , h_c = model(input_tensor, h_c)\n",
    "        predictions_scaled = predictions.cpu().numpy().reshape(1, Y_SHAPE)\n",
    "\n",
    "        predictions_raw = np.zeros_like(predictions_scaled)\n",
    "        \n",
    "        # A. No Scaler (Sin/Cos) - Przepisujemy\n",
    "        predictions_raw[:, no_scaler_y] = predictions_scaled[:, no_scaler_y]\n",
    "        \n",
    "        # B. MinMax (Delty) - Odwracamy\n",
    "        # scaler_Y_min_max oczekuje 5 cech, dajemy mu 5 cech\n",
    "        predictions_raw[:, min_max_scaler_y] = scaler_Y_min_max.inverse_transform(predictions_scaled[:, min_max_scaler_y])\n",
    "        \n",
    "        # C. Robust (Temperatury) - Odwracamy\n",
    "        # scaler_Y_robust oczekuje 4 cech, dajemy mu 4 cechy\n",
    "        predictions_raw[:, robust_scaler_y] = scaler_Y_robust.inverse_transform(predictions_scaled[:, robust_scaler_y])\n",
    "        \n",
    "    \n",
    "        return predictions_raw.flatten(), h_c\n",
    "    \n",
    "def load_data_from_db():\n",
    "    \n",
    "    \"\"\"\n",
    "    Load data so that each race is a separate sequence:\n",
    "    X = [ [state1_race1, state2_race1, ...], [state1_race2, ...] ]\n",
    "    Y = [ [next1_race1, next2_race1, ...], ... ]\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(\n",
    "        \"E:/pracadyp/Race-optimization-reinforcement-learning/data/db_states_for_regress/race_data_states.db\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT race_id, states_json FROM races ORDER BY race_id\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for race_id, states_json in rows:\n",
    "        states = json.loads(states_json)\n",
    "        data.append(states)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_x_y(data):\n",
    "    X_grouped, Y_grouped = [], []\n",
    "\n",
    "    for race in data:\n",
    "        X_seq, Y_seq = [], []\n",
    "        for i in range(len(race) - 1):\n",
    "            X_seq.append(race[i][:Y_INDEXES])  # current state\n",
    "            Y_seq.append(race[i + 1][Y_INDEXES:])  # next state\n",
    "        # dodajemy każdy wyścig osobno\n",
    "        X_grouped.append(np.array(X_seq, dtype=float))\n",
    "        Y_grouped.append(np.array(Y_seq, dtype=float))\n",
    "\n",
    "    return X_grouped, Y_grouped\n",
    "\n",
    "def create_windows(sequence_x, sequence_y, window_size, n_steps_ahead=5):\n",
    "    X, Y = [], []\n",
    "    for t in range(1, len(sequence_x)):\n",
    "        start = max(0, t - window_size)\n",
    "        window = sequence_x[start:t]\n",
    "\n",
    "        # padding na początku, jeśli okno krótsze niż window_size\n",
    "        pad_len = window_size - len(window)\n",
    "        if pad_len > 0:\n",
    "            window = np.vstack([np.zeros((pad_len, sequence_x.shape[1])), window])\n",
    "        X.append(window)\n",
    "\n",
    "        # Y: wypełniamy zerami, jeśli końcówka wyścigu ma mniej niż n_steps_ahead\n",
    "        y_window = sequence_y[t:t+n_steps_ahead]\n",
    "        if y_window.shape[0] < n_steps_ahead:\n",
    "            pad = np.zeros((n_steps_ahead - y_window.shape[0], sequence_y.shape[1]))\n",
    "            y_window = np.vstack([y_window, pad])\n",
    "        Y.append(y_window)\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def create_sliding_windows(races_x_list, races_y_list, sequence_length, step=1):\n",
    "    \"\"\"\n",
    "    Tworzy próbki (X, Y) metodą przesuwnego okna dla Teacher Forcing.\n",
    "    X = [t, t+1, ..., t+seq_len-1]\n",
    "    Y = [t+1, t+2, ..., t+seq_len]  (przesunięte o 1)\n",
    "    \"\"\"\n",
    "    all_X_samples = []\n",
    "    all_Y_samples = []\n",
    "    \n",
    "    # Pamiętaj, że race_y to już wyodrębnione 12 cech\n",
    "    # race_x to pełne 37 cech\n",
    "    \n",
    "    for race_x, race_y in zip(races_x_list, races_y_list):\n",
    "        race_length = race_x.shape[0]\n",
    "        \n",
    "        # Pętla po pojedynczym wyścigu\n",
    "        # Ostatni indeks startowy `i` musi być taki, aby `i + sequence_length`\n",
    "        # nie wyszło poza zakres dla Y (który jest przesunięty o 1)\n",
    "        for i in range(0, race_length - sequence_length, step):\n",
    "            \n",
    "            # X: Kształt (sequence_length, 37)\n",
    "            x_sample = race_x[i : i + sequence_length]\n",
    "            \n",
    "            # Y: Kształt (sequence_length, 12)\n",
    "            # Dla wejścia X w kroku 't', celem jest Y z kroku 't+1'\n",
    "            y_sample = race_y[i + 1 : i + sequence_length + 1] \n",
    "            \n",
    "            all_X_samples.append(x_sample)\n",
    "            all_Y_samples.append(y_sample)\n",
    "            \n",
    "    return np.array(all_X_samples, dtype=np.float32), np.array(all_Y_samples, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    data = load_data_from_db()\n",
    "    \n",
    "   \n",
    "    X, Y = create_x_y(data)\n",
    "    input_size = X[0].shape[1]\n",
    "    output_size = Y[0].shape[1]\n",
    "    print(input_size, output_size)\n",
    "\n",
    "    SEQUENCE_LENGTH = 800\n",
    "    STEP = 1\n",
    "\n",
    "    lr = 1e-4\n",
    "    batch_size = 128\n",
    "    num_epochs = 85\n",
    "    weight = [0.8, 1.2, 1.8, 0.2]\n",
    "   \n",
    "\n",
    "    scaler_X, scaler_Y = create_scalers(X,Y)\n",
    "\n",
    "    X_train, Y_train = scale_input(X,Y,scaler_X,scaler_Y)\n",
    "    \n",
    "    # n_steps_ahead = 5  # number of future steps to predict\n",
    "\n",
    "\n",
    "    # all_X, all_Y = [], []\n",
    "    # for race_x, race_y in zip(X_train, Y_train):  \n",
    "    #     X_r, Y_r = create_windows(race_x, race_y, window_size=30, n_steps_ahead=n_steps_ahead)\n",
    "    #     all_X.append(X_r)\n",
    "    #     all_Y.append(Y_r)\n",
    "\n",
    "    print(\"Tworzenie sampli treningowych...\")\n",
    "    X_train_samples, Y_train_samples = create_sliding_windows(\n",
    "        X_train, Y_train, SEQUENCE_LENGTH, STEP\n",
    "    )\n",
    "\n",
    "    # X_train = np.vstack(all_X)  # shape: [N_samples, window_size, n_features]\n",
    "    # Y_train = np.vstack(all_Y) \n",
    "    # all_X, all_Y = [], []\n",
    "    # for race_x, race_y in zip(X_test, Y_test):  \n",
    "    #     X_r, Y_r = create_windows(race_x, race_y, window_size=30)\n",
    "    #     all_X.append(X_r)\n",
    "    #     all_Y.append(Y_r)\n",
    "    # X_test = np.vstack(all_X)  # shape: [N_samples, window_size, n_features]\n",
    "    # Y_test = np.vstack(all_Y)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model = LSTMStatePredictor(input_size=input_size, hidden_size=256, output_size=output_size, num_layers=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train_samples, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train_samples, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "    loss_cont = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Model dostaje całą sekwencję 200 kroków\n",
    "            # i zwraca predykcje dla całej sekwencji 200 kroków\n",
    "            y_pred, _ = model(x_batch) \n",
    "            \n",
    "            # y_pred ma kształt (batch_size, SEQUENCE_LENGTH, 12)\n",
    "            \n",
    "            # Obliczamy stratę dla całej sekwencji na raz\n",
    "            # Musimy indeksować wymiar cech [:, :, ...]\n",
    "            loss_progress = loss_cont(y_pred[:, :, 0:2], y_batch[:, :, 0:2])\n",
    "            loss_fuel     = loss_cont(y_pred[:, :, 2:3], y_batch[:, :, 2:3])\n",
    "            loss_wear     = loss_cont(y_pred[:, :, 3:7], y_batch[:, :, 3:7])\n",
    "            loss_temp     = loss_cont(y_pred[:, :, 7:11], y_batch[:, :, 7:11])\n",
    "            # loss_wet      = loss_cont(y_pred[:, :, 11:], y_batch[:, :, 11:])\n",
    "            \n",
    "            # Sumujemy straty (tak jak miałeś)\n",
    "            loss = (weight[0] * loss_progress + \n",
    "                    weight[1] * loss_fuel + \n",
    "                    weight[2] * loss_wear + \n",
    "                    weight[3] * loss_temp \n",
    "                    # weight[4] * loss_wet)\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        scheduler.step(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), \"models/lstm3_model.pth\")\n",
    "    import joblib\n",
    "    joblib.dump(scaler_X, \"models/scaler3_X.pkl\")\n",
    "    joblib.dump(scaler_Y, \"models/scaler3_Y.pkl\")\n",
    "\n",
    "    print(\"✅ Model saved to models/lstm3_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db52ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c76a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używane urządzenie: cuda\n",
      "--- Rozpoczynam Eksperyment K-Fold (K=5) ---\n",
      "\n",
      "--- FOLD 1/5 ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scale_input() missing 2 required positional arguments: 'scaler_Y_min_max' and 'scaler_Y_robust'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m scaler_X_min_max, scaler_X_robust, scaler_Y_min_max, scaler_Y_robust = create_scalers(X_train_races, Y_train_races)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 3. Skalowanie obu zbiorów\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m X_train_scaled, Y_train_scaled = \u001b[43mscale_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_races\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_races\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_X_min_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_Y_min_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m X_val_scaled, Y_val_scaled     = scale_input(X_val_races, Y_val_races, scaler_X_min_max, scaler_Y_min_max)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 4. Tworzenie okien (sampli)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: scale_input() missing 2 required positional arguments: 'scaler_Y_min_max' and 'scaler_Y_robust'"
     ]
    }
   ],
   "source": [
    "data = load_data_from_db()\n",
    "X, Y = create_x_y(data) # X, Y to LISTY wyścigów\n",
    "\n",
    "# === 2. Hiperparametry Eksperymentu ===\n",
    "# Upewnij się, że znasz najkrótszy wyścig (np. 1650)\n",
    "SEQUENCE_LENGTH = 800\n",
    "STEP = 1\n",
    "\n",
    "lr = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "num_epochs = 85\n",
    "weight = [0.8, 1.2, 1.8, 0.2]\n",
    "\n",
    "\n",
    "MAX_EPOCHS = 100       # Wystarczająco dużo, aby EarlyStopping zadziałał\n",
    "PATIENCE = 15          # Cierpliwość dla EarlyStopping\n",
    "\n",
    "DROPOUT_RATE = 0.4     # Z naszej poprzedniej rozmowy\n",
    "\n",
    "# === 3. Krytyczne Ustawienia Modelu (Na podstawie naszych rozmów) ===\n",
    "# Zakładam, że Twoje wejście X ma 38 cech (z sin/cos I RaceDIST)\n",
    "# Zakładam, że Twoje wyjście Y ma 12 cech (z sin/cos, BEZ RaceDIST)\n",
    "input_size = X_SHAPE\n",
    "output_size = Y_SHAPE \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "loss_cont = nn.MSELoss()\n",
    "\n",
    "# === 4. Pętla Walidacji Krzyżowej (K-Fold) ===\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=12343)\n",
    "all_best_epochs = [] # Tu zbieramy wyniki\n",
    "\n",
    "print(f\"--- Rozpoczynam Eksperyment K-Fold (K={N_SPLITS}) ---\")\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    \n",
    "    # 1. Podział danych na listy wyścigów\n",
    "    X_train_races = [X[i] for i in train_idx]\n",
    "    Y_train_races = [Y[i] for i in train_idx]\n",
    "    X_val_races = [X[i] for i in val_idx]\n",
    "    Y_val_races = [Y[i] for i in val_idx]\n",
    "\n",
    "    # 2. Tworzenie skalerów (TYLKO na danych treningowych tego foldu)\n",
    "    scaler_X_min_max, scaler_X_robust, scaler_Y_min_max, scaler_Y_robust = create_scalers(X_train_races, Y_train_races)\n",
    "\n",
    "    # 3. Skalowanie obu zbiorów\n",
    "    X_train_scaled, Y_train_scaled = scale_input(X_train_races, Y_train_races, scaler_X_min_max,scaler_X_robust, scaler_Y_min_max, scaler_Y_robust)\n",
    "    X_val_scaled, Y_val_scaled     = scale_input(X_val_races, Y_val_races, scaler_X_min_max, scaler_X_robust, scaler_Y_min_max, scaler_Y_robust)\n",
    "    # 4. Tworzenie okien (sampli)\n",
    "    print(\"Tworzenie sampli treningowych...\")\n",
    "    X_train_samples, Y_train_samples = create_sliding_windows(X_train_scaled, Y_train_scaled, SEQUENCE_LENGTH, STEP)\n",
    "    print(\"Tworzenie sampli walidacyjnych...\")\n",
    "    X_val_samples, Y_val_samples = create_sliding_windows(X_val_scaled, Y_val_scaled, SEQUENCE_LENGTH, STEP)\n",
    "    \n",
    "    if len(X_val_samples) == 0:\n",
    "        print(f\"BŁĄD: Fold {fold+1} nie ma danych walidacyjnych. Prawdopodobnie seq_length jest za duży.\")\n",
    "        continue\n",
    "\n",
    "    # 5. Tworzenie Tensorów i DataLoaderów\n",
    "    X_train_tensor = torch.tensor(X_train_samples, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train_samples, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val_samples, dtype=torch.float32)\n",
    "    Y_val_tensor = torch.tensor(Y_val_samples, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 6. Inicjalizacja modelu i optimizera (OD NOWA dla każdego foldu)\n",
    "    model = LSTMStatePredictor(\n",
    "        input_size=input_size, \n",
    "        hidden_size=256, \n",
    "        output_size=output_size, \n",
    "        num_layers=1,\n",
    "        dropout_prob=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "\n",
    "    # 7. Zmienne do EarlyStopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_this_fold = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # === 5. Wewnętrzna pętla treningu (Epoch) ===\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, _ = model(x_batch.to(device))\n",
    "            \n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            loss_progress = loss_cont(y_pred[:, :, 0:2], y_batch[:, :, 0:2])\n",
    "            loss_fuel     = loss_cont(y_pred[:, :, 2:3], y_batch[:, :, 2:3])\n",
    "            loss_wear     = loss_cont(y_pred[:, :, 3:7], y_batch[:, :, 3:7])\n",
    "            loss_temp     = loss_cont(y_pred[:, :, 7:11], y_batch[:, :, 7:11])\n",
    "            \n",
    "            \n",
    "            # Sumujemy straty (tak jak miałeś)\n",
    "            loss = (weight[0] * loss_progress + \n",
    "                    weight[1] * loss_fuel + \n",
    "                    weight[2] * loss_wear + \n",
    "                    weight[3] * loss_temp \n",
    "                    )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # === 6. Pętla Walidacyjna (NOWA) ===\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                y_pred, _ = model(x_batch.to(device))\n",
    "\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                loss_progress = loss_cont(y_pred[:, :, 0:2], y_batch[:, :, 0:2])\n",
    "                loss_fuel     = loss_cont(y_pred[:, :, 2:3], y_batch[:, :, 2:3])\n",
    "                loss_wear     = loss_cont(y_pred[:, :, 3:7], y_batch[:, :, 3:7])\n",
    "                loss_temp     = loss_cont(y_pred[:, :, 7:11], y_batch[:, :, 7:11])\n",
    "                \n",
    "            \n",
    "                # Sumujemy straty (tak jak miałeś)\n",
    "                val_loss = (weight[0] * loss_progress + \n",
    "                        weight[1] * loss_fuel + \n",
    "                        weight[2] * loss_wear + \n",
    "                        weight[3] * loss_temp )\n",
    "                total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # === 7. Logika Schedulera i EarlyStopping ===\n",
    "        scheduler.step(avg_val_loss) # <-- Użyj avg_val_loss\n",
    "\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{MAX_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch_this_fold = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"  Early stopping w epoce {epoch + 1}.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Fold {fold + 1}: Najlepszy wynik w epoce {best_epoch_this_fold} (Val Loss: {best_val_loss:.4f})\")\n",
    "    all_best_epochs.append(best_epoch_this_fold)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Test')\n",
    "    plt.title(f'Fold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    del model\n",
    "    del optimizer\n",
    "    del X_train_samples, Y_train_samples\n",
    "    del X_val_samples, Y_val_samples\n",
    "    del train_dataset, val_dataset\n",
    "    del train_loader, val_loader\n",
    "    \n",
    "    # 2. Wymuś zwolnienie pamięci RAM\n",
    "    gc.collect()\n",
    "    \n",
    "    # 3. Wymuś zwolnienie pamięci VRAM (GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"Pamięć wyczyszczona.\\n\")\n",
    "\n",
    "# === 8. Obliczenie Mediany (Po wszystkich foldach) ===\n",
    "if all_best_epochs:\n",
    "    FINAL_EPOCH_NUMBER = int(np.median(all_best_epochs))\n",
    "    print(f\"\\n--- Eksperyment Zakończony ---\")\n",
    "    print(f\"Najlepsze epoki z każdego foldu: {all_best_epochs}\")\n",
    "    print(f\"==> Mediana optymalnych epok (Twoja 'Złota Liczba'): {FINAL_EPOCH_NUMBER}\")\n",
    "else:\n",
    "    print(\"\\n--- Eksperyment NIE POWIÓDŁ SIĘ ---\")\n",
    "    print(\"Nie zebrano żadnych wyników. Sprawdź, czy 'create_sliding_windows' zwraca dane walidacyjne.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RsOPT_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
