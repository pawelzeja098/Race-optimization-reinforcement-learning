{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09651b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'E:\\Programowanie\\Reinforcement-learning-race-simulation')\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from envs.racing_env import RacingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e11161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class  ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim,action_space):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #Actor Every action dim has its own output layer\n",
    "        self.output_layers = nn.ModuleList()\n",
    "        for n in self.action_space.nvec:\n",
    "            self.output_layers.append(nn.Linear(128, n))\n",
    "\n",
    "        #Critic\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        logits = [layer(x) for layer in self.output_layers] # list of logits for each action dim\n",
    "        \n",
    "        #rozłożenie na action dim\n",
    "        # logits_split = torch.split(self.fc2(x), self.action_space, dim=1)\n",
    "        # dla każdego wymiaru wylicz prawdopodobieństwa\n",
    "        action_probs = [F.softmax(logit, dim=1) for logit in logits]\n",
    "        state_value = self.critic(x)\n",
    "        return action_probs, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.995, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE)\n",
    "    See how good was action compared to average (value)\n",
    "    Short term and long term rewards(TD + Monte Carlo)\n",
    "    Calculating after race is done - backward through time\n",
    "    Args:\n",
    "        rewards: list of rewards\n",
    "        values: list of value estimates\n",
    "        dones: list of done flags\n",
    "        gamma: discount factor\n",
    "        lam: GAE parameter\n",
    "    Returns:\n",
    "        advantages: list of advantage estimates\n",
    "        returns: list of return estimates (advantages + values)\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [0]  # bootstrap dla końca epizodu\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "    return advantages, returns\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store trajectories for PPO updates. One race worth of data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "def ppo_update(model, optimizer, buffer, clip_eps=0.2):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (PPO) update step.\n",
    "    Clipping the policy update to avoid large updates.\n",
    "\n",
    "    \"\"\"\n",
    "    states = torch.tensor(buffer.states, dtype=torch.float32)\n",
    "    old_log_probs = torch.stack(buffer.log_probs)\n",
    "    actions = buffer.actions\n",
    "    rewards = buffer.rewards\n",
    "    values = torch.stack(buffer.values).squeeze()\n",
    "    dones = buffer.dones\n",
    "\n",
    "    advantages, returns = compute_gae(rewards, values.tolist(), dones)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # Forward pass\n",
    "    action_probs, values_new = model(states)\n",
    "\n",
    "    # Oblicz log_probs dla MultiDiscrete\n",
    "    log_probs_new = []\n",
    "    for i, probs in enumerate(action_probs):\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        acts = torch.tensor([a[i] for a in actions])\n",
    "        log_probs_new.append(dist.log_prob(acts))\n",
    "    log_probs_new = torch.stack(log_probs_new).sum(dim=0)\n",
    "\n",
    "    # PPO ratio\n",
    "    ratio = torch.exp(log_probs_new - old_log_probs.detach())\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * advantages\n",
    "\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()  # polityka\n",
    "    critic_loss = F.mse_loss(values_new.squeeze(), returns)  # Critic\n",
    "    loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def select_action(model,state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0) #add state batch dimension\n",
    "    action_probs, state_value = model(state) # get action probabilities and state value\n",
    "\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "\n",
    "    for probs in action_probs:\n",
    "        action = torch.multinomial(probs, num_samples=1).item()\n",
    "        actions.append(action)\n",
    "        log_probs.append(torch.log(probs[action]))\n",
    "\n",
    "    return actions, action_probs, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = RacingEnv()\n",
    "state_dim = env.observation_space.shape[0]\n",
    "model = ActorCritic(state_dim, env.action_space)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "buffer = RolloutBuffer()\n",
    "last_obs = []\n",
    "\n",
    "num_epochs = 1000\n",
    "steps_per_epoch = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    obs = env.reset()\n",
    "    for step in range(steps_per_epoch):\n",
    "        actions, log_prob, value = select_action(model, obs)\n",
    "        next_obs, reward, done, _ = env.step(actions)\n",
    "        last_obs = next_obs\n",
    "\n",
    "        # zapis danych do buffer\n",
    "        buffer.states.append(obs)\n",
    "        buffer.actions.append(actions)\n",
    "        buffer.log_probs.append(log_prob)\n",
    "        buffer.values.append(value)\n",
    "        buffer.rewards.append(torch.tensor(reward, dtype=torch.float32))\n",
    "        buffer.dones.append(done)\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "\n",
    "    # PPO update po rollout\n",
    "    ppo_update(model, optimizer, buffer)\n",
    "    buffer.clear()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        total_reward = sum([r.item() for r in buffer.rewards])\n",
    "        print(f\"Epoch {epoch}, total reward {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
